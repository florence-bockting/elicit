{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fd307-f420-4f8c-bbb3-b910dd1d5957",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_probability as tfp\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "tfd = tfp.distributions \n",
    "\n",
    "from functions.user_interface.input_functions import param, model, target, loss, expert, optimization, prior_elicitation\n",
    "from user_input.custom_functions import Normal_log"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e171b32a-f3bf-4c42-a7d8-6dc74b87de82",
   "metadata": {},
   "source": [
    "# Poisson Regression Model\n",
    "\n",
    "## Background: Case Study\n",
    "For demonstration purposes, we adapt an example from Johnson et al. (2022), which investigates the number of LGBTQ+ anti-discrimination laws in each US state. The distribution of these laws is assumed to follow a Poisson distribution, with the rate of such laws being influenced by demographic and voting trend. The demographic trend is quantified by the percentage of a stateâ€™s residents living in urban areas, ranging from 38.7\\% to 94.7\\%. Additionally, the voting trend is represented by\n",
    "historical voting patterns in presidential elections, categorizing each state as consistently voting for the Democratic or Republican candidate or being a Swing state. We employ a Poisson regression model including one treatment-coded categorical predictor: the voting trend. This predictor has three levels: Democrats, Republicans, and Swing, with Democrats serving as the reference category. Furthermore, the model incorporates one continuous predictor: the demographic trend, measured as\n",
    "a percentage. \n",
    "\n",
    "**Reference**\n",
    "\n",
    "Johnson, A. A., Ott, M. Q., & Dogucu, M. (2022). *Bayes rules!: An introduction to applied Bayesian modeling*. CRC Press.\n",
    "\n",
    "## Data generating model\n",
    "\\begin{align*}\n",
    "    y_i &\\sim \\text{Poisson}(\\theta_i)\\\\\n",
    "    \\text{log}(\\theta_i) &= \\beta_0 + \\beta_1x_1 + \\beta_2 x_2 + \\beta_3 x_3\\\\\n",
    "    \\beta_k &\\sim \\text{Normal}(\\mu_k, \\sigma_k) \\quad \\text{for }k=0,\\ldots,3\\\\\n",
    "\\end{align*}\n",
    "Here, $y_i$ is the number of counts for observation $i = 1, \\ldots, N$. The counts follow a Poisson distribution with rate $\\theta_i$. The rate parameter is predicted by a linear combination of two predictors: the continuous predictor $x_1$ with slope $\\beta_1$ and a three-level factor represented by the coefficients $\\beta_2$ and $\\beta_3$ for both contrasts $x_2$ and $x_3$. The logged average count $y$ is denoted by $\\beta_0$. All regression coefficients are assumed to have normal prior distributions with mean $\\mu_k$ and standard deviation $\\sigma_k$ for $k = 0, \\ldots, 3$. The log-link function maps $\\theta_i$ to the scale of the linear predictor. The main goal is to learn the hyperparameters $\\lambda = (\\mu_k , \\sigma_k)$ based on expert knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598b5bdf-a242-4121-832c-96f936a962fc",
   "metadata": {},
   "source": [
    "## Methodology: Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ea76a9-2971-4827-a2f1-dcfc48586e50",
   "metadata": {},
   "source": [
    "+ PriorSamples($\\lambda = (\\mu_k, \\sigma_k)$):\n",
    "\\begin{align*}\n",
    "\\{\\beta_k\\}_s &\\sim \\text{Normal}(\\mu_k, \\exp\\{\\sigma_k\\})\\\\\n",
    "\\end{align*}\n",
    "+ Generator($\\beta_k$):\n",
    "\\begin{align*}\n",
    "\\{\\text{log}(\\theta_i)\\}_s &= \\{\\beta_k\\}_s \\times X_i\\\\\n",
    "\\{y_i\\}_s &\\sim \\text{Poisson}(\\{\\theta_i\\}_s)\\\\\n",
    "\\end{align*}\n",
    "+ Targets($\\{y_j\\}_s$, $\\{\\bar{y}_G\\}_s$):\n",
    "\\begin{align*}\n",
    "\\{y_j\\}_s &= \\{y_j\\}_s \\quad \\text{ for } j = 1,11,17,22,35,44\\\\\n",
    "\\{\\bar{y}_G\\}_s &= \\frac{1}{\\mid G \\mid} \\sum_{i\\in G}\\mathbb 1_G (\\{y_i\\}_s) \\quad \\text{ with G = Democratic, Republican, Swing}\\\\\n",
    "\\end{align*}\n",
    "+ Elicits($\\{y_{j}\\}_s, \\{R^2\\}_s$)\n",
    "\\begin{align*}\n",
    "\\text{Histogram-based:}& \\quad \\{y_j\\}_s = \\{y_j\\}_s \\quad \\text{ for } j = 1,11,17,22,35,44\\\\\n",
    "\\text{Quantile-based:}& \\quad Q_p^{G} = Q_{p}^{G} \\{\\bar{y}_G\\}_s \\quad \\text{G = Democratic, Republican, Swing, } p = 0.1, \\ldots, 0.9\\\\\n",
    "\\end{align*}\n",
    "\n",
    "## Setting up the method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36980b47-a5f6-45bd-896a-91979836c160",
   "metadata": {},
   "source": [
    "### Model parameters\n",
    "First, we need to specify the parameters in the generative model using the `param()` function which requires the following input:\n",
    "+ `name`: the name of the model parameter\n",
    "    + In our Poisson model we have four model parameters which we will denote as *b0*,*b1*,*b2*, and *b3*\n",
    "+ `family`: the prior distribution family\n",
    "    + We assume a Normal prior distribution for each model parameter \n",
    "    + We created a wrapper around the Normal distribution as implemented in tensorflow-probability in order to learn the $\\sigma$-hyperparameter on the log-scale.\n",
    "+ `hyperparams_dict`: a dictionary including the name of the hyperparameter values and the initial value for the learning algorithm\n",
    "    + We use distributions from which an initial value is drawn.\n",
    "    + When a hyperparameter is learned on the log-scale, we indicate this by writing \"log_\" before the actual name of the hyperparameter, such as *log_sigma0*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c3e876-ab62-4a01-b094-91a5a6dd09dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_log = Normal_log()\n",
    "    \n",
    "def model_params():  \n",
    "    return (\n",
    "        param(name = \"b0\", \n",
    "              family = normal_log, \n",
    "              hyperparams_dict = {\"mu0\": tfd.Uniform(1.,2.5), \n",
    "                                  \"log_sigma0\": tfd.Uniform(-2.,-5.)}\n",
    "              ),\n",
    "        param(name = \"b1\", \n",
    "              family = normal_log, \n",
    "              hyperparams_dict = {\"mu1\": tfd.Uniform(0.,0.5), \n",
    "                                  \"log_sigma1\": tfd.Uniform(-2.,-5.)}\n",
    "              ),\n",
    "        param(name = \"b2\", \n",
    "              family = normal_log, \n",
    "              hyperparams_dict = {\"mu2\": tfd.Uniform(-1.,-1.5), \n",
    "                                  \"log_sigma2\": tfd.Uniform(-2.,-5.)}\n",
    "              ),\n",
    "        param(name = \"b3\", \n",
    "              family = normal_log, \n",
    "              hyperparams_dict = {\"mu3\": tfd.Uniform(-0.5,-1.), \n",
    "                                  \"log_sigma3\": tfd.Uniform(-2.,-5.)}\n",
    "              )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c2ee7d-62ad-467c-bb91-3f746626cfc0",
   "metadata": {},
   "source": [
    "### Expert data or ground truth for method validation\n",
    "\n",
    "Next, we define the `expert()` function which represents the data input based on which the method has to learn. \n",
    "This can be either \n",
    "+ data from an expert (in this case the argument `data` expects a string with the location to the expert data) or\n",
    "+ an expected ground truth in which case we simulate once from the method using a pre-defined hyperparameter vector $\\lambda^*$ and learn then on this simulated data (in this case we need to set the argument `simulate_data = True` and specify the true hyperparameter values in `simulator_specs`.\n",
    "    \n",
    "The second approach is helpful when we want to validate our method and check whether the implementation is correct. Because it allows us to check whether we can recover an expected ground truth under *ideal circumstances*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7283f6b-e112-4ff3-bb1f-f6cbd1639b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expert_input():\n",
    "    return expert(data = None,\n",
    "                  simulate_data = True,\n",
    "                  simulator_specs = {\n",
    "                      \"b0\": tfd.Normal(2.91, 0.07),\n",
    "                      \"b1\": tfd.Normal(0.23, 0.05),\n",
    "                      \"b2\": tfd.Normal(-1.51, 0.135),\n",
    "                      \"b3\": tfd.Normal(-0.61, 0.105)\n",
    "                      })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5126561-4f51-4ad6-8684-9d910adde091",
   "metadata": {},
   "source": [
    "### Generative model\n",
    "\n",
    "Now, we can define our generative model from which data should be simulated. We specified the model already formally. \n",
    "For implementation purposes a particular *input-output* structure is required:\n",
    "+ input:\n",
    "    + (required) `prior_samples`: samples drawn from the prior distributions \n",
    "    + (optional) `design_matrix`: design matrix used for the regression model\n",
    "+ output:\n",
    "    + (required) `likelihood`: model likelihood\n",
    "    + (required) `ypred`: prior predictions (if likelihood is discrete `ypred=None` as it will be approximated using the Softmax-Gumble method)\n",
    "    + (required) `epred`: linear predictor\n",
    "    + (optional) `prior_samples`: we use it here again as output for easier follow-up computations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfac528c-0b56-4fba-a280-2fcee78c834c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativePoissonModel(tf.Module):\n",
    "    def __call__(self, \n",
    "                prior_samples,        \n",
    "                design_matrix,       \n",
    "                **kwargs        \n",
    "                ):  \n",
    "        \n",
    "        # linear predictor\n",
    "        theta = design_matrix @ tf.expand_dims(prior_samples, -1)\n",
    "        \n",
    "        # map linear predictor to theta\n",
    "        epred = tf.exp(theta)\n",
    "        \n",
    "        # define likelihood\n",
    "        likelihood = tfd.Poisson(\n",
    "            rate = epred\n",
    "        )\n",
    "        \n",
    "        return dict(likelihood = likelihood,     \n",
    "                    ypred = None,   \n",
    "                    epred = epred[:,:,:,0],\n",
    "                    prior_samples = prior_samples               \n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946e5b66-e60e-49ec-a7a9-d24f5a58d7fc",
   "metadata": {},
   "source": [
    "After having specified the generative model, we load all information into the `model()` function which requires the following specifications:\n",
    "+ `generative_model`: the class of the generative model (callable)\n",
    "+ `additional_model_args`: specifications of model arguments used as input that are not prior samples\n",
    "    + in this example we have additionally the *design_matrix*. The argument name must match with the argument of the generative model and the value is then the corresponding value (here the design matrix)\n",
    "+ `discrete_likelihood`: whether likelihood is discrete or not; if *True* the softmax-gumble method will be used in order to approximate `ypred`\n",
    "+ `softmax_gumble_specs`: Additional settings required by the softmax-gumble method.\n",
    "    + `temperature`: (default) $1.$ (for smaller values tending towards zero the Gumbel-Softmax distr. is equiv. to the categorical distr.)\n",
    "    + `upper_threshold`: (required) if likelihood is unbounded, the upper threshold argument specifies where to truncate the distribution. Currently, we have only implemented the case where distributions lack an upper bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028fbf48-412f-47d6-8a9e-e2781a68551d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from user_input.generative_models import GenerativePoissonModel\n",
    "from user_input.design_matrices import load_design_matrix_equality\n",
    "\n",
    "design_matrix = load_design_matrix_equality(\"standardize\", selected_obs = [0, 13, 14, 35, 37, 48])\n",
    "\n",
    "def generative_model():\n",
    "    return model(GenerativePoissonModel,\n",
    "                 additional_model_args = {\n",
    "                     \"design_matrix\": design_matrix},\n",
    "                 discrete_likelihood = True,\n",
    "                 softmax_gumble_specs = {\"temperature\": 1.,\n",
    "                                         \"upper_threshold\": 80}\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98297725-a927-4662-abd9-de9b48cfb4e6",
   "metadata": {},
   "source": [
    "### Target quantities and elicitation techniques\n",
    "In the next step, we can define the target quantities and the corresponding elicitation technique. Both is specified through the `target()` function which has the following options:\n",
    "+ `name`: name of the target quantity\n",
    "    + if you want to use a target quantity which is already specified in the generative model, then you the value for `name` must match with the output argument from the generative model. For this example, `ypred` is taken directly from the generative model.\n",
    "+ `custom_target_function`: (optional) it is possible to compute custom target quantities from the output of the generative model. Here we compute for example the group means based on the prior predictions. If a custom target function is used the following specifications have to be done:\n",
    "    + `function`: the respective custom function (callable)\n",
    "    + `additional_args`: in case the custom function takes as arguments parameters that are not in the output of the generative model, these parameters need to be specified here in form of a dictionary. The argument name is used as key and the respective value. \n",
    "+ `elicitation_method`: currently available elicitation methods are *histogram*, *moments*, and *quantiles*\n",
    "    + some of the elicitation methods require additional specifications:\n",
    "        + *moments* requires the additional argument `moments_specs` which takes a tuple with the moments that should be elicited e.g. (\"mean\", \"sd\")\n",
    "        + *quantiles* requires the additional argument `quantiles_specs` which takes a tuple with the percentages that should be elicited e.g. (25, 50, 75)\n",
    "+ `loss_components`: specifies the form of the loss components. Possible values are *by-group*, *by-stats*,*all*. (Detailed description will follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ae0a29-1957-4389-a122-8c4beffaef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from user_input.custom_functions import custom_group_means\n",
    "    \n",
    "def target_quantities():\n",
    "    return (\n",
    "        target(name = \"ypred\",\n",
    "               elicitation_method = \"histogram\",\n",
    "               loss_components = \"by-group\"\n",
    "               ),\n",
    "        target(name = \"group_means\",\n",
    "                elicitation_method = \"quantiles\",\n",
    "                quantiles_specs = (25, 50, 75),\n",
    "                custom_target_function = {\n",
    "                    \"function\": custom_group_means,\n",
    "                    \"additional_args\": {\"design_matrix\": design_matrix,\n",
    "                                        \"factor_indices\": [0,2,3]}\n",
    "                    },\n",
    "                loss_components = \"by-group\"\n",
    "                )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0c282-b948-41a5-93ee-2b46a9a8ea06",
   "metadata": {},
   "source": [
    "### Loss function and optimization settings\n",
    "\n",
    "The loss function used to compute the discrepancy between the expert and the model-implied elicited statistics is specified via the `loss()` function which has the following arguments:\n",
    "+ `loss_function`: the discrepancy measure (string: *mmd-energy* or callable)\n",
    "+ `loss_weighting`: if a weighting scheme for the multiobjective loss function should be used.\n",
    "\n",
    "For the optimization settings, we consider for the moment only batch stochastic gradient descent for which we need to specify the optimizer using the `optimization()` function, which takes two arguments:\n",
    "+ `optimizer`: The optimizer that should be used for the current case study (here we use the Adam optimizer)\n",
    "+ `optimizer_specs`: If further specifications of the optimizer are needed, they are specified here. For the Adam optimizer we need for example an initial learning rate. The additional keyword arguments are specified in form of a dictionary with the key matching the argument name. In this case study we use a cosine decay learning rate schedule with restarts. And we clip the gradient norm at 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ab9d20-de9a-46b7-9a19-183a15187bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function():\n",
    "    return loss(loss_function = \"mmd-energy\",\n",
    "                loss_weighting = None\n",
    "                )\n",
    "\n",
    "def optimization_settings():\n",
    "    return optimization(\n",
    "                    optimizer = tf.keras.optimizers.Adam,\n",
    "                    optimizer_specs = {\n",
    "                        \"learning_rate\": tf.keras.optimizers.schedules.CosineDecayRestarts(\n",
    "                            0.01, 50),\n",
    "                        \"clipnorm\": 1.0}\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72b5fd-57b7-400c-8a4a-cf47eb8ae600",
   "metadata": {},
   "source": [
    "### Run the learning algorithm\n",
    "Finally we can wrap everything up and run the learning algorithm. Therefore we specify the last hyperparameter needed by the learning algorithm with the following `prior_elicitation()` function:\n",
    "+ `method`: method for learning the prior distributions (currently available: *parametric_prior*)\n",
    "+ `sim_id`: unique name of model (also used for saving results)\n",
    "+ `B`: batch size\n",
    "+ `rep`: number of simulations from the prior distributions\n",
    "+ `seed`: seed for the current simulation\n",
    "+ `burnin`: runs before learning starts in order to find good initial values (if drawn randomly)\n",
    "+ `epochs`: number of learning cycles until learning stops\n",
    "+ `output_path`: file location for storing results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab488302-6d92-45ba-a15e-4ffc4c3db21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_elicitation(\n",
    "        method = \"parametric_prior\",\n",
    "        sim_id = \"pois_34765558\",\n",
    "        B = 128,\n",
    "        rep = 300,\n",
    "        seed = 34765558,\n",
    "        burnin = 10,\n",
    "        epochs = 700,\n",
    "        output_path = \"results\",\n",
    "        model_params = model_params,\n",
    "        expert_input = expert_input,\n",
    "        generative_model = generative_model,\n",
    "        target_quantities = target_quantities,\n",
    "        loss_function = loss_function,\n",
    "        optimization_settings = optimization_settings,\n",
    "        log_info = 0,\n",
    "        print_info=True,\n",
    "        view_ep = 1\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed71a341-eb79-42d0-a526-5d424ec419f5",
   "metadata": {},
   "source": [
    "## Evaluating method performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3521acbe-be8f-48a5-ad54-01a46594f189",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sim_scripts.plot_learned_prior import learned_prior_pois\n",
    "from sim_scripts.plot_diagnostics import diagnostics_pois\n",
    "from validation.plot_pois_res import plot_results_overview_param\n",
    "\n",
    "path = \"elicit/simulations/case_studies/sim_results/\"\n",
    "file = \"pois_34765682_200\"\n",
    "selected_obs = [0, 13, 14, 35, 37, 48]\n",
    "true_values = [2.91, 0.07, 0.23, 0.05, -1.51, 0.135, -0.61, 0.105]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f83e10-0da9-4ea5-b43c-2246939cdd8e",
   "metadata": {},
   "source": [
    "### Convergence diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0871158-010e-4551-b55f-0735687bf5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot diagnostics\n",
    "diagnostics_pois(path, file, save_fig = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef89683-34f2-4666-8150-75bd993cfc6a",
   "metadata": {},
   "source": [
    "### Learned prior distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72342cd-31aa-48ba-83a2-4d95f8bceda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learned prior distributions\n",
    "learned_prior_pois(path, file, selected_obs, true_values, \n",
    "                    last_vals = 30, save_fig = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5526175-adaf-4668-bfb3-b1c3dfe0fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot overview of simulation results (for appendix)\n",
    "plot_results_overview_param(path, file, \"Poisson model - parametric prior\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
